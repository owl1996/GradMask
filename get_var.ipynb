{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxUsgxYVtiI2",
        "outputId": "dd567948-03a0-4b69-c2cc-398acd92eaa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seuils gradient pour linear.weight: 1\n",
            "Seuils gradient pour linear.bias: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.func import vmap, grad, functional_call  # Nouvelle API PyTorch 2.x\n",
        "\n",
        "# Modèle simple\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.linear = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Initialisation\n",
        "model = SimpleModel()\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Génération de données\n",
        "batch_size = 5\n",
        "x = torch.randn(batch_size, 50)  # Pas besoin de requires_grad=True ici\n",
        "y = torch.randn(batch_size, 1)\n",
        "\n",
        "# Fonction qui retourne la loss pour un seul échantillon\n",
        "def compute_loss(params, buffers, x_sample, y_sample):\n",
        "    output = functional_call(model, params, (x_sample.unsqueeze(0),))  # Passage avant avec params donnés\n",
        "    return criterion(output, y_sample.unsqueeze(0))  # Perte\n",
        "\n",
        "# Obtenir les paramètres du modèle sous forme de dictionnaire\n",
        "params = {name: param for name, param in model.named_parameters()}\n",
        "buffers = {name: buffer for name, buffer in model.named_buffers()}  # Pour les modules comme BatchNorm\n",
        "\n",
        "# Calcul du gradient de la loss par rapport aux paramètres\n",
        "compute_grad = grad(compute_loss)\n",
        "\n",
        "# Vectoriser pour tout le batch\n",
        "batched_grads = vmap(compute_grad, (None, None, 0, 0))(params, buffers, x, y)\n",
        "\n",
        "# Calcul de la variance des gradients\n",
        "gradient_variance = {name: torch.var(batched_grads[name], dim=0)\n",
        "                     for name in batched_grads.keys()}\n",
        "\n",
        "# Calcul de la moyenne des gradients\n",
        "gradient_mean = {name: torch.mean(batched_grads[name], dim=0)\n",
        "                     for name in batched_grads.keys()}\n",
        "\n",
        "# Affichage\n",
        "p = 0.8\n",
        "for name, var in gradient_variance.items():\n",
        "    mean = gradient_mean[name]\n",
        "    print(f\"Seuils gradient pour {name}: {torch.sum(torch.distributions.Normal(0, 1).cdf(torch.abs(mean) / var) > p)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ffcv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
